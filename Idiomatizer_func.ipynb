{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18760,"status":"ok","timestamp":1673951523495,"user":{"displayName":"Balázs Szalontai","userId":"18338656504074219239"},"user_tz":-60},"id":"uVN5NAil1707","outputId":"ba5a42d0-8432-4444-d0af-d528925b5956"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10239,"status":"ok","timestamp":1673951533727,"user":{"displayName":"Balázs Szalontai","userId":"18338656504074219239"},"user_tz":-60},"id":"12V1gYOD2Lx9","outputId":"dfea1c74-657e-4cf2-8ed5-fdbb316b4884"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting import_ipynb\n","  Downloading import_ipynb-0.1.4-py3-none-any.whl (4.1 kB)\n","Requirement already satisfied: IPython in /usr/local/lib/python3.8/dist-packages (from import_ipynb) (7.9.0)\n","Requirement already satisfied: nbformat in /usr/local/lib/python3.8/dist-packages (from import_ipynb) (5.7.1)\n","Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.8/dist-packages (from IPython->import_ipynb) (57.4.0)\n","Requirement already satisfied: pexpect in /usr/local/lib/python3.8/dist-packages (from IPython->import_ipynb) (4.8.0)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.8/dist-packages (from IPython->import_ipynb) (4.4.2)\n","Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.8/dist-packages (from IPython->import_ipynb) (5.7.1)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.8/dist-packages (from IPython->import_ipynb) (0.7.5)\n","Requirement already satisfied: backcall in /usr/local/lib/python3.8/dist-packages (from IPython->import_ipynb) (0.2.0)\n","Requirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from IPython->import_ipynb) (2.0.10)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.8/dist-packages (from IPython->import_ipynb) (2.6.1)\n","Collecting jedi>=0.10\n","  Downloading jedi-0.18.2-py2.py3-none-any.whl (1.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: jupyter-core in /usr/local/lib/python3.8/dist-packages (from nbformat->import_ipynb) (5.1.2)\n","Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.8/dist-packages (from nbformat->import_ipynb) (2.16.2)\n","Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.8/dist-packages (from nbformat->import_ipynb) (4.3.3)\n","Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.8/dist-packages (from jedi>=0.10->IPython->import_ipynb) (0.8.3)\n","Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema>=2.6->nbformat->import_ipynb) (0.19.3)\n","Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema>=2.6->nbformat->import_ipynb) (22.2.0)\n","Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema>=2.6->nbformat->import_ipynb) (5.10.2)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.8/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->IPython->import_ipynb) (1.15.0)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.8/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->IPython->import_ipynb) (0.2.5)\n","Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.8/dist-packages (from jupyter-core->nbformat->import_ipynb) (2.6.2)\n","Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.8/dist-packages (from pexpect->IPython->import_ipynb) (0.7.0)\n","Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.8/dist-packages (from importlib-resources>=1.4.0->jsonschema>=2.6->nbformat->import_ipynb) (3.11.0)\n","Installing collected packages: jedi, import_ipynb\n","Successfully installed import_ipynb-0.1.4 jedi-0.18.2\n","importing Jupyter notebook from /content/drive/MyDrive/tidierpp/Common_functions.ipynb\n","\n"]}],"source":["!pip3 install import_ipynb\n","import import_ipynb\n","import drive.MyDrive.harp.Common_functions as Common_functions # Make sure to adjust file paths"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-uAVTAcd46t5"},"outputs":[],"source":["from keras.layers import Input, Embedding, Bidirectional, LSTM, GRU, Concatenate, TimeDistributed, Dense\n","from keras.models import Model\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","import numpy as np\n","from typing import List\n","from collections import deque"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z9pMGS1fDZ7B"},"outputs":[],"source":["def tokens_to_code(tokens, unk_dict = None, indent_info = None, throw_when_unk_failure=False):\n","    #replace general vars to actual vars\n","    if unk_dict is not None:\n","        rev_unk_dict = Common_functions.reverse_dict(unk_dict)\n","        rev_unk_dict['comma'] = ','\n","        rev_unk_dict['dot'] = '.'\n","        tokens_with_original_vars = []\n","        for token in tokens:\n","            try:\n","                tokens_with_original_vars.append(rev_unk_dict[token])\n","            except KeyError:\n","                if any(token.startswith(e) and token != e for e in ['atom', 'var', 'integer']):\n","                    return ''\n","                    #raise Exception('Not all tokens could be converted back to identifier/variable/integer')\n","                tokens_with_original_vars.append(token)\n","\n","    #if any(token in ['atom', 'var', 'integer'] for token in tokens_with_original_vars):\n","    #    print(tokens_with_original_vars)\n","    #    raise Exception('Not all tokens could be converted back to identifier/variable/integer')\n","\n","    if indent_info is None:\n","        indent_info = (0,'    ')\n","\n","    #convert token sequence to code string\n","    del_space_before_tokens = ']())}.,:;'\n","    no_space_after_tokens= '[({.\\n\\t-'\n","    code_string = ''\n","    for token in tokens_with_original_vars:\n","        if token in del_space_before_tokens:\n","            try:\n","                if code_string[-1] == ' ' and code_string[-2] != ' ':\n","                    code_string = code_string[:-1]\n","            except IndexError:\n","                pass\n","        if token == '\\t':\n","            code_string += indent_info[1]\n","        else:\n","            code_string += token\n","        if token not in no_space_after_tokens:\n","            code_string += ' '\n","\n","    code_lines = code_string.splitlines()\n","    code_lines = [indent_info[0] * indent_info[1] + line for line in code_lines]\n","    code_string = '\\n'.join(code_lines)\n","    return code_string"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OJ8QUDiWmajp"},"outputs":[],"source":["def generate_encoder_decoder_inputs_outputs(num_tokens, latent_dim, embedding_dim, MAX_LENGTH_NONIDIOMATIC, MAX_LENGTH_IDIOMATIC,\n","                                            encoder_rnn_num = 2, stacked_encoder = False, dropout_rate : int = 0.2, use_gru : bool = False):\n","    encoder_inputs = Input(shape=(MAX_LENGTH_NONIDIOMATIC,), name='encoder_inputs')\n","\n","    encoder_rnn_dim = latent_dim//encoder_rnn_num\n","\n","    emb_layer = Embedding(num_tokens, embedding_dim, mask_zero = True, name='enc_emb_layer')\n","    enc_emb = emb_layer(encoder_inputs)\n","\n","    rnn_layer = GRU if use_gru else LSTM\n","\n","    encoder_rnns = [\n","        (Bidirectional(rnn_layer(encoder_rnn_dim, return_sequences=True, return_state=True, dropout=dropout_rate, name=f'rnn{i+1}'), name=f'bidirectional{i+1}')\n","         if not stacked_encoder else\n","         Bidirectional(rnn_layer(encoder_rnn_dim, return_sequences=True, return_state=False, dropout=dropout_rate, name=f'rnn{i+1}'), name=f'bidirectional{i+1}'))\n","        for i in range(encoder_rnn_num)\n","    ]\n","\n","    encoder_outputs = [\n","        encoder_rnn(enc_emb)\n","        for encoder_rnn in encoder_rnns\n","    ]\n","\n","    if stacked_encoder:\n","        second_layer_rnns = [\n","            Bidirectional(rnn_layer(encoder_rnn_dim, return_sequences=True, return_state=True, dropout=dropout_rate, name=f'stacked_rnn{i+1}'))\n","            for i in range(encoder_rnn_num)\n","        ]\n","        stacked_encoder_outputs = [\n","            encoder_rnn(encoder_outputs[i])\n","            for i, encoder_rnn in enumerate(second_layer_rnns)\n","        ]\n","        encoder_outputs = stacked_encoder_outputs\n","\n","    encoder_outputs, *states = zip(*encoder_outputs)\n","    encoder_outputs = Concatenate()(encoder_outputs)\n","    if use_gru:\n","        states_a, states_b = states\n","        state = Concatenate()([*states_a, *states_b])\n","        encoder_states  = [state]\n","    else:\n","        forward_h, forward_c, backward_h, backward_c = states\n","        state_h = Concatenate(name='concat_state_h')([*forward_h, *backward_h])\n","        state_c = Concatenate(name='concat_state_c')([*forward_c, *backward_c])\n","        encoder_states = [state_h, state_c]\n","\n","    decoder_inputs = Input(shape=(MAX_LENGTH_IDIOMATIC, ), name='decoder_inputs')\n","\n","    dec_emb = emb_layer(decoder_inputs)\n","\n","    decoder_rnn = rnn_layer(latent_dim*2, return_sequences=True, return_state=True, name='decoder_rnn')\n","\n","    decoder_outputs, *_ = decoder_rnn(dec_emb, initial_state=encoder_states)\n","\n","    decoder_encoder_attention_layer = Common_functions.AttentionLayer(name='decoder_encoder_attention')\n","    decoder_encoder_attention       = decoder_encoder_attention_layer([encoder_outputs,decoder_outputs])\n","\n","    decoder_dense = TimeDistributed(Dense(num_tokens, activation='softmax'),name='final_dense')\n","    decoder_outputs = decoder_dense(decoder_encoder_attention)\n","\n","    return (encoder_inputs, decoder_inputs, encoder_outputs, decoder_outputs, encoder_states, emb_layer, decoder_rnn, decoder_encoder_attention_layer, decoder_dense)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q2mkJEXYmiFp"},"outputs":[],"source":["def generate_encoder_decoder_model(encoder_inputs, encoder_states, encoder_outputs,\n","                                   latent_dim, embedding_dim, emb_layer, decoder_inputs,\n","                                   decoder_rnn, decoder_encoder_attention_layer,\n","                                   decoder_dense, MAX_LENGTH_NONIDIOMATIC, use_gru = False):\n","    encoder_model = Model([encoder_inputs], [*encoder_states, encoder_outputs])\n","\n","    encoder_output_input   = Input(shape=(MAX_LENGTH_NONIDIOMATIC,latent_dim*2), name='encoder_output_input')\n","    if use_gru:\n","        decoder_state_input    = Input(shape=(latent_dim*2,), name='decoder_state_input_h')\n","        decoder_state_input    = [decoder_state_input]\n","    else:\n","        decoder_state_input_h = Input(shape=(latent_dim*2,), name='decoder_state_input_h')\n","        decoder_state_input_c = Input(shape=(latent_dim*2,), name='decoder_state_input_c')\n","        decoder_state_input   = [decoder_state_input_h, decoder_state_input_c]\n","\n","    dec_emb2 = emb_layer(decoder_inputs)\n","    if use_gru:\n","        decoder_outputs2, state = decoder_rnn(dec_emb2, initial_state=decoder_state_input)\n","        decoder_states2 = [state]\n","    else:\n","        decoder_outputs2, state_h2, state_c2 = decoder_rnn(dec_emb2, initial_state=decoder_state_input)\n","        decoder_states2 = [state_h2, state_c2]\n","\n","    decoder_encoder_attention2 = decoder_encoder_attention_layer([encoder_output_input, decoder_outputs2])\n","\n","    decoder_outputs2 = decoder_dense(decoder_encoder_attention2)\n","\n","    decoder_model = Model(\n","        [encoder_output_input, decoder_inputs] + decoder_state_input,\n","        [decoder_outputs2] + decoder_states2)\n","\n","    return (encoder_model, decoder_model)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kPFKt2Rdryqe"},"outputs":[],"source":["def decode_sequences(input_seqs : List,\n","                     encoder_model, decoder_model,\n","                     word2index_seq2seq, index2word_seq2seq,\n","                     MAX_LENGTH_IDIOMATIC,\n","                     batch_size : int = 1):\n","    *state, enc_output = encoder_model.predict(input_seqs)\n","    target_seqs = np.zeros((batch_size,1))\n","    for i in range(batch_size):\n","        target_seqs[i, 0] = word2index_seq2seq['BOS']\n","    decoded_sequences = [ [] for _ in range(batch_size) ]\n","    stop_conditions = [False] * batch_size\n","    while not all(stop_conditions):\n","        output_tokens, *hc = decoder_model.predict([enc_output, target_seqs, *state])\n","        sampled_token_indexes = [\n","            np.argmax(output_tokens[i, -1, :])\n","            for i in range(batch_size)\n","        ]\n","        sampled_words = [\n","            index2word_seq2seq[sampled_token_index]\n","            for sampled_token_index in sampled_token_indexes\n","        ]\n","        for i in range(batch_size):\n","            if not stop_conditions[i]:\n","                decoded_sequences[i].append(sampled_words[i])\n","            if (sampled_words[i] == 'EOS' or len(decoded_sequences[i]) > MAX_LENGTH_IDIOMATIC):\n","                stop_conditions[i] = True\n","        target_seqs = np.zeros((batch_size,1))\n","        for i in range(batch_size):\n","            target_seqs[i, 0] = sampled_token_indexes[i]\n","        state = hc\n","    decoded_sequences = [\n","        decoded_sequence[:-1] if decoded_sequence[-1] == 'EOS' else decoded_sequence\n","        for decoded_sequence in decoded_sequences\n","    ]\n","    return decoded_sequences"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hjIM2F5j6clI"},"outputs":[],"source":["def idiomatize(codes,\n","               encoder_model, decoder_model,\n","               word2index_seq2seq, index2word_seq2seq,\n","               MAX_LENGTH_IDIOMATIC, MAX_LENGTH_NONIDIOMATIC,\n","               throw_when_unk_failure=False,\n","               batch_size : int = 2**7,\n","               to_string : bool = True, pprint : bool = False):\n","    if not codes:\n","        return []\n","    if isinstance(codes[0],str):\n","        input_seqs = Common_functions.tokenize_code(codes)\n","        input_seqs = [ Common_functions.process_tokenized_code(input_seq) for input_seq in input_seqs ]\n","    elif isinstance(codes[0],list) or isinstance(codes[0],tuple):\n","        input_seqs = codes\n","    else:\n","        assert False\n","    input_seqs, unk_dicts = zip(*input_seqs)\n","    input_seqs = [\n","        [word2index_seq2seq[token] for token in input_seq if token in word2index_seq2seq.keys()]\n","        for input_seq in input_seqs\n","    ]\n","    input_seqs = pad_sequences(input_seqs, maxlen=MAX_LENGTH_NONIDIOMATIC, padding='post', value=0., dtype='float32')\n","\n","    decoded_sequences = deque()\n","    test_set_len = len(input_seqs)\n","    fits_into_batches = test_set_len % batch_size == 0\n","    for i in range(test_set_len//batch_size + (0 if fits_into_batches else 1)):\n","        fr, to = batch_size*i, batch_size*(i+1)\n","        bs = len(input_seqs[fr:to])\n","        assert bs <= batch_size\n","        decoded_sequences.extend(\n","            decode_sequences(input_seqs[fr:to],\n","                            encoder_model, decoder_model,\n","                            word2index_seq2seq, index2word_seq2seq,\n","                            MAX_LENGTH_IDIOMATIC, bs)\n","        )\n","\n","    assert test_set_len == len(decoded_sequences)\n","    if not to_string:\n","        return decoded_sequences\n","\n","    #decoded_sequences = np.array(decoded_sequences)\n","\n","\n","    idiomatic_codes = [\n","        tokens_to_code(decoded_sequence, unk_dict, throw_when_unk_failure=throw_when_unk_failure)\n","        for decoded_sequence, unk_dict in zip(decoded_sequences, unk_dicts)\n","    ]\n","    assert len(idiomatic_codes) == test_set_len\n","\n","    if pprint:\n","        pretty_idiomatic_codes = Common_functions.erl_pretty_printer(idiomatic_codes)\n","        idiomatic_codes = [\n","            pretty_code\n","            if (code and\n","                       code.replace(' ','').replace('\\t','').replace('\\n','') ==\n","                pretty_code.replace(' ','').replace('\\t','').replace('\\n',''))\n","            else code\n","            for code, pretty_code in zip(idiomatic_codes, pretty_idiomatic_codes)\n","        ]\n","\n","        assert len(pretty_idiomatic_codes) == test_set_len\n","\n","    return idiomatic_codes"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyN2nPHHY14/cE0qCi+Jdh2D","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
